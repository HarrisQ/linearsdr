---
title: "Linear SDR for scalar Y, Ex1"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Linear SDR for scalar Y, Ex1}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(ggplot2)
library(linearsdr)

library("doParallel")
library("foreach")
print( paste( as.character(detectCores()), "cores detected" ) );
# Create cluster with desired number of cores
cl <- makePSOCKcluster(detectCores()-1)
# Register cluster
doParallel::registerDoParallel(cl)
# Find out how many cores are being used
print( paste( as.character(getDoParWorkers() ), "cores registered" ) )
# stopCluster(cl)
```


In this vignette, we demonstrate the available methods for linear Sufficient Dimension Reduction methods when working with continuous responses. The datasets are all accessible from the UCI repository [here](https://archive.ics.uci.edu/ml/index.php).

## Airfoil Noise

The airfoil dataset is available [here](https://archive.ics.uci.edu/ml/datasets/airfoil+self-noise) and consists of 5 predictors and one response. The variables are measurements taken during airfoil tests and the response is the sound pressure level. Two of the predictors are discrete, which violate some theoretical assumptions for the methods applied, but we will see that the methods can still work well. A pairs plot illustrates the discreteness for some of the predictors. We randomly sample a subset of $500$ for our estimation since the computation for MAVE can get quite slow.


```{r}
data('airfoil_datta', package="linearsdr")
# summary(airfoil_datta)

X = as.matrix(airfoil_datta[,1:5])
Y = c(airfoil_datta[,6])

pairs(X)

```

### Sliced Inverse Regression (SIR)

We choose the number of slices to be $5$ and plot only the first sufficient predictor against the response.

```{r}
# Sliced Inverse Regression

b_hat_sir = sir(x=X, y=Y, nslices = 5, d=2, ytype = "continuous" )$beta


linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_sir[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='SIR 1', v_lab='Y',
                        main_lab= paste0('SIR'), size=2.5)

```


### Sliced Average Variance Estimator (SAVE)

We choose the number of slices to be $5$ and plot only the first sufficient predictor against the response.

```{r}
# Sliced Average Variance Estimator

b_hat_save = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = "continuous" )$beta


linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T, 
                        ytype="continuous",
                        h_lab='DR 1', v_lab='Y',
                        main_lab= paste0('DR'), size=2.5)




```

### Directional Regression (DR)

We choose the number of slices to be $5$ and plot only the first sufficient predictor against the response.

```{r}
# Directional Regression

b_hat_dr = dr(x=X, y=Y, nslices = 5, d=2, ytype = "continuous" )$beta


linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr[,1]), y_on_axis=T, 
                        ytype="continuous",
                        h_lab='DR 1', v_lab='Y',
                        main_lab= paste0('DR'), size=2.5)




```

### Outer Product of Gradients (OPG)

We use a Gaussian kernel for the local linear weights and standardize the predictors so that the kernel choice is more appropriate. The bandwidth is set to $5$ and we plot only the first sufficient predictor against the response. We parallelize the computation out of convenience here. 

```{r}
# OPG Estimate 


X_std=(sapply(1:dim(X)[2], FUN= function(j)
  center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);


b_hat_opg = opcg(x=X_std, y=Y, bw = 5, d=2, ytype = "continuous", 
                 method= "cg", parallelize = T )


linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='OPG 1', v_lab='Y',
                        main_lab= paste0('OPG'), size=2.5)

```


### Minimum Average Variance Estimation (MAVE)

We use a Gaussian kernel for the local linear weights and standardize the predictors so that the kernel choice is more appropriate. For the starting value of the $\beta$ matrix, We use a $p \times d$ matrix with the upper $d \times d$ block being the identity. The bandwidth is set to $5$ and we plot only the first sufficient predictor against the response.
![](../man/figures/ex1_mave.png)

```{r}
# MAVE Estimate

# The code is commented out to speed up compiling of the documentation. 

# n=length(Y)
# 
# start_time1 = Sys.time();
# b_hat_made = made(x=(X_std), matrix(Y, nrow = n, ncol=1), bw=5, d=2,
#                   ytype='continuous',
#                   method=list(opcg="cg", made="cg"), B_mat = NULL,
#                   parallelize=T,
#                   control_list=list(print_iter=T, max_iter_made=5) )
# 
# end_time1 = Sys.time(); end_time1 - start_time1;
#
# # [1] "MADE: euc_dist dist is" "0.331801141953547"      "1"
# # [1] "MADE: euc_dist dist is" "0.0112107472862494"     "2"
# # [1] "MADE: euc_dist dist is" "0.0102038542211796"     "3"
# # [1] "MADE: euc_dist dist is" "0.00497290505768543"    "4"
# # [1] "MADE: euc_dist dist is" "0.00261423206170839"    "5"
# # [1] "0 - non-convergence"
# # Time difference of 22.35525 mins
#  
# 
# plot saved image instead
# mave_plot=linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_made[,1]), y_on_axis=T,
#                                   ytype="continuous",
#                                   h_lab='MAVE 1', v_lab='Y',
#                                   main_lab= paste0('MAVE'), size=2.5)
# 
# linearsdr:::save_sdr_plot(mave_plot,filename = paste0('../man/figures/ex1_mave.png'),
#                           width = 900, height = 450, units = "px", pointsize = 12,
#                           bg = "white",  res = 100)

```


## Crime Rate and Community Survey (Regularized Inverse Methods)


```{r}
data('crime_datta', package="linearsdr")
dim(crime_datta)

X = crime_datta[, 1:( dim(crime_datta)[2] - 1) ]
Y = crime_datta[, dim(crime_datta)[2]]

p = dim(X)[2]; n=dim(X)[1];
 

```
### Sliced Inverse Regression (SIR)

```{r}
# Sliced Inverse Regression

b_hat_sir = sir(x=X, y=Y, nslices = 10, d=2, ytype = "continuous",
                lambda = 0)$beta


linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_sir[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='SIR 1', v_lab='Y',
                        main_lab= paste0('SIR'), size=2.5)
```

### Sliced Average Variance Estimator (SAVE)


```{r}
# Sliced Average Variance Estimator

b_hat_save = save_sdr(x=X, y=Y, nslices = 10, d=2, ytype = "continuous",
              lambda = 0)$beta

b_hat_save_reg = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = "continuous",
                  lambda = .1)$beta

par(mar=c(1, 0, 0, 1), mfrow=c(1,1))

linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='SAVE 1', v_lab='Y',
                        main_lab= paste0('SAVE'), size=2.5)

linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save_reg[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='SAVE 1', v_lab='Y',
                        main_lab= paste0('Regularized SAVE'), size=2.5)




```
### Directional Regression (DR)

```{r}
# Directional Regression

b_hat_dr = dr(x=X, y=Y, nslices = 10, d=2, ytype = "continuous",
              lambda = 0)$beta


b_hat_dr_reg = dr(x=X, y=Y, nslices = 10, d=2, ytype = "continuous",
                  lambda = .1)$beta


linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='DR 1', v_lab='Y',
                        main_lab= paste0('DR'), size=1)

linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr_reg[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='DR 1', v_lab='Y',
                        main_lab= paste0('Regularized DR'), size=2.5)




```

### Outer Product of Gradients (OPG)

```{r}

X_std=(sapply(1:dim(X)[2], FUN= function(j)
  center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);

b_hat_opg = opcg(x=X_std, y=Y, bw = 7, d=2, ytype = "continuous", 
                 method= "cg", parallelize = T )


linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='OPG 1', v_lab='Y',
                        main_lab= paste0('OPG'), size=2.5)

```

### Minimum Average Variance Estimation (MAVE)







## Energy Efficiency (Multivariate $Y$)

```{r}
data('energy_datta', package="linearsdr")
# summary(energy_datta)
# dim(energy_datta)

X = as.matrix(energy_datta[,1:8])
Y1 = as.numeric(energy_datta$Y1)
Y2 = as.numeric(energy_datta$Y2)

pairs(X)
```

### Sliced Inverse Regression (SAVE)
```{r}

b_hat_sir = sir(x=X, y=Y1, nslices = 10, d=2, ytype = "continuous" )$beta


linearsdr:::ggplot_fsdr(Y1, t((X)%*%b_hat_sir[,1:2]), y_on_axis=F,
                        ytype="continuous",
                        h_lab='SIR 1', v_lab='SIR 2',
                        main_lab= paste0('SIR'), size=3)


```


### Sliced Average Variance Estimator (SAVE)

We choose the number of slices to be $5$ and plot only the first sufficient predictor against the response.

```{r}
# Sliced Average Variance Estimator

b_hat_save = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = "continuous" )$beta


linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T, 
                        ytype="continuous",
                        h_lab='DR 1', v_lab='Y',
                        main_lab= paste0('DR'), size=2.5)




```

### Directional Regression (DR)
```{r}

b_hat_dr = dr(x=X, y=Y1, nslices = 10, d=2, ytype = "continuous" )$beta

linearsdr:::ggplot_fsdr(Y1, t((X)%*%b_hat_dr[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='DR 1', v_lab='DR 2',
                        main_lab= paste0('DR'), size=3)




```

### Outer Product of Gradients (OPG)
```{r}
# Parallelization not run because of computational time. 

X_std=(sapply(1:dim(X)[2], FUN= function(j)
  center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);

b_hat_opg = opcg(x=X_std, y=Y1, bw = .75, d=2, ytype = "continuous", 
                 method= "cg", parallelize = T )


linearsdr:::ggplot_fsdr(Y1, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,
                        ytype="continuous",
                        h_lab='OPG 1', v_lab='OPG 2',
                        main_lab= paste0('OPG'), size=3)

```
