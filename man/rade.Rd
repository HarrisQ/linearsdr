% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rade.R
\name{rade}
\alias{rade}
\title{Regularized Regressing Average Derivative Estimates}
\usage{
rade(
  x_matrix,
  y_matrix,
  d,
  bw,
  ytype = "continuous",
  method = "newton",
  parallelize = F,
  l2_pen = 0,
  l1_pen = 0,
  control_list = list()
)
}
\arguments{
\item{x_matrix}{a 'pxn' matrix of predictors;}

\item{y_matrix}{a 'mxn' matrix response}

\item{d}{specified the reduced dimension}

\item{bw}{the bandwidth parameter for the kernel; the default kernel is gaussian}

\item{ytype}{specify the response as 'continuous', 'multinomial', or 'ordinal'}

\item{method}{Default is 'newton'. Select the method for optimization. Strongly 
suggest the conjugate gradient method 'cg' when 'p' of moderate size.}

\item{parallelize}{Default is False; to run in parallel, you will need to have
foreach and some parallel backend loaded; parallelization is strongly recommended
and encouraged.}

\item{l2_pen}{Default is '0', i.e. no ridge penalty.}

\item{l1_pen}{Default is '0', i.e no LASSO Penalty. The LASSO estimation is carried out
through the 'glmnet' package. To select the L1 penalty through Cross-Validation in 
glmnet, set 'l1_pen = -1'. Otherwise, provide a sequence of penalty values (glmnet 
strongly discourages supplying a single value.)}

\item{control_list}{a list of control parameters for the Newton-Raphson 
or Conjugate Gradient methods
\itemize{
  \item opcg - A 'pxd' matrix that estimates a basis for the central subspace.
  \item opcg_wls - A 'pxd' matrix that estimates a basis for the central subspace based 
  on the initial value of the optimization problem; useful for examining bad starting 
  values.
  \item cand_mat - A list that contains both the candidate matrix for OPCG and for
  the initial value; this is used in other functions for order determination
  \item gradients - The estimated local gradients; used in regularization of OPCG
  \item weights - The kernel weights in the local-linear GLM. 
}}
}
\value{

}
\description{
Regularized Regressing Average Derivative Estimates
}
\examples{

}
