[{"path":"https://harrisq.github.io/linearsdr/articles/linearsdr.html","id":"r-markdown","dir":"Articles","previous_headings":"","what":"R Markdown","title":"linearsdr overview","text":"R Markdown document. Markdown simple formatting syntax authoring HTML, PDF, MS Word documents. details using R Markdown see http://rmarkdown.rstudio.com. click Knit button document generated includes content well output embedded R code chunks within document. can embed R code chunk like :","code":"summary(cars) ##      speed           dist        ##  Min.   : 4.0   Min.   :  2.00   ##  1st Qu.:12.0   1st Qu.: 26.00   ##  Median :15.0   Median : 36.00   ##  Mean   :15.4   Mean   : 42.98   ##  3rd Qu.:19.0   3rd Qu.: 56.00   ##  Max.   :25.0   Max.   :120.00"},{"path":"https://harrisq.github.io/linearsdr/articles/linearsdr.html","id":"including-plots","dir":"Articles","previous_headings":"","what":"Including Plots","title":"linearsdr overview","text":"can also embed plots, example:  Note echo = FALSE parameter added code chunk prevent printing R code generated plot.","code":""},{"path":"https://harrisq.github.io/linearsdr/articles/Tuning OPCG.html","id":"r-markdown","dir":"Articles","previous_headings":"","what":"R Markdown","title":"Tuning OPCG","text":"R Markdown document. Markdown simple formatting syntax authoring HTML, PDF, MS Word documents. details using R Markdown see http://rmarkdown.rstudio.com. click Knit button document generated includes content well output embedded R code chunks within document. can embed R code chunk like :","code":"summary(cars) ##      speed           dist        ##  Min.   : 4.0   Min.   :  2.00   ##  1st Qu.:12.0   1st Qu.: 26.00   ##  Median :15.0   Median : 36.00   ##  Mean   :15.4   Mean   : 42.98   ##  3rd Qu.:19.0   3rd Qu.: 56.00   ##  Max.   :25.0   Max.   :120.00"},{"path":"https://harrisq.github.io/linearsdr/articles/Tuning OPCG.html","id":"including-plots","dir":"Articles","previous_headings":"","what":"Including Plots","title":"Tuning OPCG","text":"can also embed plots, example:  Note echo = FALSE parameter added code chunk prevent printing R code generated plot.","code":""},{"path":"https://harrisq.github.io/linearsdr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors","text":"Harris Quach. Author, maintainer.","code":""},{"path":"https://harrisq.github.io/linearsdr/index.html","id":"linear-sufficient-dimension-reduction","dir":"","previous_headings":"","what":"Linear SDR","title":"Linear SDR","text":"Contains code forthcoming paper “Generalized Forward Regression Sufficient Dimension Reduction Multinomial Response”. package can installed running: devtools::install_github(\"HarrisQ/linearsdr\"); ‘NAMESPACE - oxy’ original NAMESPACE file prior started using rOxygen kept reference. ‘NAMESPACE - oxy’ NAMESPACE file generated rOxygen.","code":""},{"path":"https://harrisq.github.io/linearsdr/index.html","id":"current-state-of-package","dir":"","previous_headings":"","what":"Current State of Package:","title":"Linear SDR","text":"Forward Linear SDR methods: OPG/OPCG, MAVE/MADE, Tuning OPCG Inverse Linear SDR methods: SIR, DR, SAVE options regularization Clean OPG/OPCG MAVE/MADE code (Maybe) Limit function exports just ones people use (Maybe) Clean Regularized OPG/OPCG (RADE) Code Write Vignettes/Examples Finish code Multivariate Continuous Response Write Documentation functions","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Directional Regression — dr","title":"Directional Regression — dr","text":"conducts directional regression (DR) Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Directional Regression — dr","text":"","code":"dr(x, y, nslices, d, ytype, std = T, lambda = 0)"},{"path":"https://harrisq.github.io/linearsdr/reference/dr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Directional Regression — dr","text":"x 'p x n' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' std predictors standardized? Default 'TRUE' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/dr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Directional Regression — dr","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix DR; used functions order determination.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/dr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Directional Regression — dr","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005). apply idea SAVE, find context analogous regularization works.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/kfold_km_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"K-fold Tuning with K-means — kfold_km_tuning","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"implements tuning procedure SDR classification problems forth coming paper Quach Li (2021).","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/kfold_km_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"","code":"kfold_km_tuning(   h_list,   k,   x_datta,   y_datta,   d,   ytype,   class_labels,   n_cpc,   method = \"newton\",   std = \"none\",   parallelize = F,   control_list = list(),   iter.max = 100,   nstart = 100 )"},{"path":"https://harrisq.github.io/linearsdr/reference/kfold_km_tuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"h_list  k  x_datta  y_datta  d specified reduced dimension ytype specify response 'continuous', 'multinomial', 'ordinal' class_labels  n_cpc  method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. control_list list control parameters Newton-Raphson Conjugate Gradient methods iter.max  nstart","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/kfold_km_tuning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"list containing estimate candidate matrix. opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/kfold_km_tuning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/made.html","id":null,"dir":"Reference","previous_headings":"","what":"Minimum Average Deviance Estimation — made","title":"Minimum Average Deviance Estimation — made","text":"implements Outer Product Canonical Gradients (OPCG) forth coming paper Quach Li (2021).","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/made.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Minimum Average Deviance Estimation — made","text":"","code":"made(   x_matrix,   y_matrix,   d,   bw,   lambda = 0,   B_mat = NULL,   ytype = \"continuous\",   method = list(opcg = \"newton\", made = \"newton\"),   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://harrisq.github.io/linearsdr/reference/made.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minimum Average Deviance Estimation — made","text":"x_matrix 'pxn' matrix predictors; y_matrix 'mxn' matrix response d specified reduced dimension bw bandwidth parameter kernel; default kernel gaussian B_mat  ytype specify response 'continuous', 'cat', 'ord-cat' method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. r_mat  control_list list control parameters Newton-Raphson Conjugate Gradient methods opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/made.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minimum Average Deviance Estimation — made","text":"list containing estimate candidate matrix. opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/made.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Minimum Average Deviance Estimation — made","text":"version MADE differs Adragni currently available continuous, cat, ord-cat response far. scalar continuous response, estimation identical OPG. kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/opcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Outer Product of Canonical Gradients — opcg","title":"Outer Product of Canonical Gradients — opcg","text":"implements Outer Product Canonical Gradients (OPCG) forth coming paper Quach Li (2021).","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/opcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Outer Product of Canonical Gradients — opcg","text":"","code":"opcg(   x_matrix,   y_matrix,   d,   bw,   lambda = 0,   ytype = \"continuous\",   method = \"newton\",   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://harrisq.github.io/linearsdr/reference/opcg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outer Product of Canonical Gradients — opcg","text":"x_matrix 'pxn' matrix predictors; y_matrix 'mxn' matrix response d specified reduced dimension bw bandwidth parameter kernel; default kernel gaussian ytype specify response 'continuous', 'multinomial', 'ordinal' method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. control_list list control parameters Newton-Raphson Conjugate Gradient methods opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/opcg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outer Product of Canonical Gradients — opcg","text":"'pxd' matrix estimates basis central subspace based estimated local gradients","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/opcg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Outer Product of Canonical Gradients — opcg","text":"kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/save_sdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Sliced Average Variance Estimation — save_sdr","title":"Sliced Average Variance Estimation — save_sdr","text":"conducts sliced average variance estimation (SAVE) Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/save_sdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Sliced Average Variance Estimation — save_sdr","text":"","code":"save_sdr(x, y, nslices, d, ytype, std = T, lambda = 0)"},{"path":"https://harrisq.github.io/linearsdr/reference/save_sdr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sliced Average Variance Estimation — save_sdr","text":"x 'p x n' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' std predictors standardized? Default 'TRUE' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/save_sdr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sliced Average Variance Estimation — save_sdr","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix SAVE; used functions order determination.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/save_sdr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sliced Average Variance Estimation — save_sdr","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005). apply idea SAVE, find context analogous regularization works.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/sir.html","id":null,"dir":"Reference","previous_headings":"","what":"Sliced Inverse Regression — sir","title":"Sliced Inverse Regression — sir","text":"conducts sliced inverse regression Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/sir.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Sliced Inverse Regression — sir","text":"","code":"sir(x, y, nslices, d, ytype, std = T, lambda = 0)"},{"path":"https://harrisq.github.io/linearsdr/reference/sir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sliced Inverse Regression — sir","text":"x 'p x n' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' std predictors standardized? Default 'TRUE' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/sir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sliced Inverse Regression — sir","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix SIR; used functions order determination.","code":""},{"path":"https://harrisq.github.io/linearsdr/reference/sir.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sliced Inverse Regression — sir","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005).","code":""}]
