[{"path":"https://github.com/HarrisQ/linearsdr/articles/linearsdr.html","id":"introduction-to-linear-sdr","dir":"Articles","previous_headings":"","what":"Introduction to Linear SDR","title":"Linear SDR","text":"Sufficient Dimension Reduction (SDR) subfield statistics focused finding lower dimensional summaries relational information interest variables preserved. mathematical terms, suppose random elements \\(Y\\) \\(X\\), interested seeking lower-dimension summary \\(X\\) information \\(Y\\) available \\(X\\) preserved. want summary statistic \\(X\\), denoted \\(s(X)\\), \\(Y\\) statistically independent \\(X\\) given \\(s(X)\\), .e. \\(Y\\) indep \\(X | s(X)\\). Linear SDR interested finding lower dimension summaries \\(X\\) linear functions \\(X\\), \\(s(X) = \\beta^{\\top} X\\) \\(p \\times d\\) matrix \\(\\beta\\), \\(d\\) much smaller \\(p\\). package covers methods can categorized two branches SDR: Inverse Regression methods Forward Regression Methods.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/articles/linearsdr.html","id":"inverse-methods","dir":"Articles","previous_headings":"","what":"Inverse Methods","title":"Linear SDR","text":"Broadly speaking, inverse regression methods SDR involve regressing predictor variable \\(X\\) onto response variable \\(Y\\). Methods involve quantities \\(E(X|Y)\\). Methods package fall category Sliced Inverse Regression (SIR), Sliced Average Variance Estimation (SAVE) Directional Regression (DR). \\(p\\) large, may find situation regularization necessary, methods SIR, SAVE DR come option place Tikhonov regularizer sample covariance matrix. regularization inverse methods SDR similar spirit Zhong et. al. (2007).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/articles/linearsdr.html","id":"forward-methods","dir":"Articles","previous_headings":"","what":"Forward Methods","title":"Linear SDR","text":"Forward regression methods SDR involve conventional approach regressing response variable \\(Y\\) onto predictor variable \\(X\\). Methods involve quantities \\(E(Y|X)\\). Methods package fall category Outer Product Gradients (OPG), Minimum Average Variance Estimation (MAVE), Outer Product Canonical Gradients (OPCG) Minimum Average Deviance Estimator (MADE). Options L1 L2 regularizing OPCG may available upon request.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"a-first-example","dir":"Articles","previous_headings":"","what":"A First Example","title":"Linear SDR for scalar Y, Ex1","text":"airfoil dataset available consists 5 predictors one response. variables measurements taken airfoil tests response sound pressure level. Two predictors discrete, violate theoretical assumptions methods applied, see methods can still work well. pairs plot illustrates discreteness predictors. randomly sample subset \\(500\\) estimation since computation MAVE can get quite slow.","code":"data('airfoil_datta', package=\"linearsdr\") # summary(airfoil_datta)  X = as.matrix(airfoil_datta[,1:5]) Y = c(airfoil_datta[,6])  pairs(X)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-inverse-regression-sir","dir":"Articles","previous_headings":"A First Example","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Inverse Regression  b_hat_sir = sir(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_sir[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='Y',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-average-variance-estimator-save","dir":"Articles","previous_headings":"A First Example","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Average Variance Estimator  b_hat_save = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"directional-regression-dr","dir":"Articles","previous_headings":"A First Example","what":"Directional Regression (DR)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Directional Regression  b_hat_dr = dr(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"outer-product-of-gradients-opg","dir":"Articles","previous_headings":"A First Example","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for scalar Y, Ex1","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. bandwidth set \\(5\\) plot first sufficient predictor response. parallelize computation convenience .","code":"# OPG Estimate    X_std=(sapply(1:dim(X)[2], FUN= function(j)   center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);   b_hat_opg = opcg(x=X_std, y=Y, bw = 5, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )   linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='Y',                         main_lab= paste0('OPG'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"minimum-average-variance-estimation-mave","dir":"Articles","previous_headings":"A First Example","what":"Minimum Average Variance Estimation (MAVE)","title":"Linear SDR for scalar Y, Ex1","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. starting value \\(\\beta\\) matrix, use \\(p \\times d\\) matrix upper \\(d \\times d\\) block identity. bandwidth set \\(5\\) plot first sufficient predictor response.","code":"# MAVE Estimate  # The code is commented out to speed up compiling of the documentation.   # n=length(Y) #  # start_time1 = Sys.time(); # b_hat_made = made(x=(X_std), matrix(Y, nrow = n, ncol=1), bw=5, d=2, #                   ytype='continuous', #                   method=list(opcg=\"cg\", made=\"cg\"), B_mat = NULL, #                   parallelize=T, #                   control_list=list(print_iter=T, max_iter_made=5) ) #  # end_time1 = Sys.time(); end_time1 - start_time1; # # # [1] \"MADE: euc_dist dist is\" \"0.331801141953547\"      \"1\" # # [1] \"MADE: euc_dist dist is\" \"0.0112107472862494\"     \"2\" # # [1] \"MADE: euc_dist dist is\" \"0.0102038542211796\"     \"3\" # # [1] \"MADE: euc_dist dist is\" \"0.00497290505768543\"    \"4\" # # [1] \"MADE: euc_dist dist is\" \"0.00261423206170839\"    \"5\" # # [1] \"0 - non-convergence\" # # Time difference of 22.35525 mins #   #  # plot saved image instead # mave_plot=linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_made[,1]), y_on_axis=T, #                                   ytype=\"continuous\", #                                   h_lab='MAVE 1', v_lab='Y', #                                   main_lab= paste0('MAVE'), size=2.5) #  # linearsdr:::save_sdr_plot(mave_plot,filename = paste0('../man/figures/ex1_mave.png'), #                           width = 900, height = 450, units = \"px\", pointsize = 12, #                           bg = \"white\",  res = 100)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"regularized-inverse-methods","dir":"Articles","previous_headings":"","what":"Regularized Inverse Methods","title":"Linear SDR for scalar Y, Ex1","text":"Crime Rate Community Survey","code":"data('crime_datta', package=\"linearsdr\") dim(crime_datta) #> [1] 500 100  X = crime_datta[, 1:( dim(crime_datta)[2] - 1) ] Y = crime_datta[, dim(crime_datta)[2]]  p = dim(X)[2]; n=dim(X)[1];"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-inverse-regression-sir-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for scalar Y, Ex1","text":"","code":"# Sliced Inverse Regression  b_hat_sir = sir(x=X, y=Y, nslices = 10, d=2, ytype = \"continuous\",                 lambda = 0)$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_sir[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='Y',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-average-variance-estimator-save-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for scalar Y, Ex1","text":"### Directional Regression (DR)","code":"# Sliced Average Variance Estimator  b_hat_save = save_sdr(x=X, y=Y, nslices = 10, d=2, ytype = \"continuous\",               lambda = 0)$beta  b_hat_save_reg = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\",                   lambda = .1)$beta  par(mar=c(1, 0, 0, 1), mfrow=c(1,1))  linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='Y',                         main_lab= paste0('SAVE'), size=2.5) linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save_reg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='Y',                         main_lab= paste0('Regularized SAVE'), size=2.5) # Directional Regression  b_hat_dr = dr(x=X, y=Y, nslices = 10, d=2, ytype = \"continuous\",               lambda = 0)$beta   b_hat_dr_reg = dr(x=X, y=Y, nslices = 10, d=2, ytype = \"continuous\",                   lambda = .1)$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=1) linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr_reg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('Regularized DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"outer-product-of-gradients-opg-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for scalar Y, Ex1","text":"","code":"X_std=(sapply(1:dim(X)[2], FUN= function(j)   center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);  b_hat_opg = opcg(x=X_std, y=Y, bw = 7, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )   linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='Y',                         main_lab= paste0('OPG'), size=2.5)"},{"path":[]},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"multivariate-y","dir":"Articles","previous_headings":"","what":"Multivariate \\(Y\\)","title":"Linear SDR for scalar Y, Ex1","text":"Energy Efficiency","code":"data('energy_datta', package=\"linearsdr\") # summary(energy_datta) # dim(energy_datta)  X = as.matrix(energy_datta[,1:8]) Y1 = as.numeric(energy_datta$Y1) Y2 = as.numeric(energy_datta$Y2)  pairs(X)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-inverse-regression-save","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Sliced Inverse Regression (SAVE)","title":"Linear SDR for scalar Y, Ex1","text":"","code":"b_hat_sir = sir(x=X, y=Y1, nslices = 10, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y1, t((X)%*%b_hat_sir[,1:2]), y_on_axis=F,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='SIR 2',                         main_lab= paste0('SIR'), size=3)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-average-variance-estimator-save-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Average Variance Estimator  b_hat_save = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5) #> Warning in rbind(y_datta, x_datta): number of columns of result is not a #> multiple of vector length (arg 1)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"directional-regression-dr-1","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Directional Regression (DR)","title":"Linear SDR for scalar Y, Ex1","text":"","code":"b_hat_dr = dr(x=X, y=Y1, nslices = 10, d=2, ytype = \"continuous\" )$beta  linearsdr:::ggplot_fsdr(Y1, t((X)%*%b_hat_dr[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='DR 2',                         main_lab= paste0('DR'), size=3)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"outer-product-of-gradients-opg-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for scalar Y, Ex1","text":"","code":"# Parallelization not run because of computational time.   X_std=(sapply(1:dim(X)[2], FUN= function(j)   center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);  b_hat_opg = opcg(x=X_std, y=Y1, bw = .75, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )   linearsdr:::ggplot_fsdr(Y1, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='OPG 2',                         main_lab= paste0('OPG'), size=3)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cat.html","id":"categorical-responses","dir":"Articles","previous_headings":"","what":"Categorical Responses","title":"Linear SDR for categorical Y","text":"demonstrate methods categorical data handwritten pen digits dataset available , consists 16 predictors representing written features one categorical response digits 0-9. randomly sample 1000 observations training set 1000 observations testing set. two samples combined ‘pendigits_datta’ matrix, first 1000 training last 1000 testing. Inverse methods, since response categorical, number slices taken equal number levels response. , take 10 slices SIR, SAVE DR. methods, plot first three predictors , coloring labels illustrate separation digits.","code":"data('pendigits_datta', package=\"linearsdr\")  X1 = as.matrix(pendigits_datta[1:1000,1:16]) Y1 = c(pendigits_datta[1:1000,17])  X1_test = as.matrix(pendigits_datta[1001:2000,1:16]) Y1_test = c(pendigits_datta[1001:2000,17])"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cat.html","id":"sliced-inverse-regression-sir","dir":"Articles","previous_headings":"Categorical Responses","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for categorical Y","text":"SIR, can recover 9 directions since using 10 slices, set \\(d=9\\).","code":"# Sliced Inverse Regression  b_hat_sir1 = sir(x=X1, y=Y1, nslices = 10, d=9, ytype = \"categorical\" )$beta   linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_sir1[,c(1,2)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='SIR 1', v_lab='SIR2',                         main_lab= paste0('SIR'), size=2.5)  linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_sir1[,c(2,3)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='SIR 2', v_lab='SIR3',                         main_lab= paste0('SIR'), size=2.5)  linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_sir1[,c(1,3)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='SIR 1', v_lab='SIR3',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cat.html","id":"sliced-average-variance-estimator-save","dir":"Articles","previous_headings":"Categorical Responses","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for categorical Y","text":"SAVE, constraint \\(d\\), set \\(9\\) convenience. effect estimation.","code":"# Sliced Average Variance Estimator  b_hat_save1 = save_sdr(x=X1, y=Y1, nslices = 10, d=9, ytype = \"categorical\" )$beta   linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_save1[,c(1,2)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='SAVE 1', v_lab='SAVE 2',                         main_lab= paste0('SAVE'), size=2.5)  linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_save1[,c(2,3)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='SAVE 2', v_lab='SAVE 3',                         main_lab= paste0('SAVE'), size=2.5)  linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_save1[,c(1,3)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='SAVE 1', v_lab='SAVE3',                         main_lab= paste0('SAVE'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cat.html","id":"directional-regression-dr","dir":"Articles","previous_headings":"Categorical Responses","what":"Directional Regression (DR)","title":"Linear SDR for categorical Y","text":"Similarly DR, constraint \\(d\\), set \\(9\\) convenience. effect estimation.","code":"# Directional Regression  b_hat_dr1 = dr(x=X1, y=Y1, nslices = 10, d=9, ytype = \"categorical\" )$beta   linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_dr1[,c(1,2)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='DR 1', v_lab='DR 2',                         main_lab= paste0('DR'), size=2.5)  linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_dr1[,c(2,3)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='DR 2', v_lab='DR 3',                         main_lab= paste0('DR'), size=2.5)  linearsdr:::ggplot_fsdr(Y1_test, t((X1_test)%*%b_hat_dr1[,c(1,3)]), y_on_axis=F,                         ytype=\"multinomial\",                         h_lab='DR 1', v_lab='DR 3',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cat.html","id":"outer-product-of-canonical-gradients-opcg","dir":"Articles","previous_headings":"Categorical Responses","what":"Outer Product of Canonical Gradients (OPCG)","title":"Linear SDR for categorical Y","text":"OPCG, use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. bandwidth set \\(1.42\\) using Tuning method OPCG proposed paper. parallelize computation make OPCG relatively competitive inverse methods terms computation. convenience, set \\(d=9\\); effect estimation. code run due compiling speed site.","code":"# OPG Estimate     # X1_std=(sapply(1:dim(X1)[2], FUN= function(j) #   center_cpp(X1[,j], NULL) ) )%*%matpower_cpp(cov((X1)) , -1/2); #  # X1_test_std=(sapply(1:dim(X1_test)[2], FUN= function(j) #   center_cpp(X1_test[,j], NULL) ) )%*%matpower_cpp(cov((X1_test)) , -1/2); #  #  #  # b_hat_opcg1 = opcg(x=X1_std, y=Y1, bw = 1.42, d=9, ytype = \"cat\", #                   method= \"cg\", parallelize = T ) #  # opcg_plot1_1=linearsdr:::ggplot_fsdr(Y1_test, t((X1_test_std)%*%b_hat_opcg1[,c(1,2)]), #                                      y_on_axis=F, #                                      ytype=\"multinomial\", #                                      h_lab='OPCG 1', v_lab='OPCG 2', #                                      main_lab= paste0('OPCG'), size=2.5) #  # opcg_plot1_2=linearsdr:::ggplot_fsdr(Y1_test, t((X1_test_std)%*%b_hat_opcg1[,c(2,3)]), #                                      y_on_axis=F, #                                       ytype=\"multinomial\", #                                       h_lab='OPCG 2', v_lab='OPCG 3', #                                       main_lab= paste0('OPCG'), size=2.5) #  # opcg_plot1_3=linearsdr:::ggplot_fsdr(Y1_test, t((X1_test_std)%*%b_hat_opcg1[,c(1,3)]), #                                      y_on_axis=F, #                                       ytype=\"multinomial\", #                                       h_lab='OPCG 1', v_lab='OPCG 3', #                                       main_lab= paste0('OPCG'), size=2.5) #  # linearsdr:::save_sdr_plot(opcg_plot1_1, #                           filename = paste0('../man/figures/cat/ex1_opcg1.png'), #                           width = 800, height = 450, units = \"px\", pointsize = 12, #                           bg = \"white\",  res = 125)   knitr::include_graphics('../man/figures/cat/ex1_opcg1.png') knitr::include_graphics('../man/figures/cat/ex1_opcg2.png') knitr::include_graphics('../man/figures/cat/ex1_opcg3.png')"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cat.html","id":"minimum-average-variance-estimation-mave","dir":"Articles","previous_headings":"Categorical Responses","what":"Minimum Average Variance Estimation (MAVE)","title":"Linear SDR for categorical Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. starting value \\(\\beta\\) matrix, use \\(p \\times d\\) matrix upper \\(d \\times d\\) block identity. bandwidth set \\(1.42\\) set \\(d=9\\) since estimated dimension according Predictor Augmentation method.","code":"# MAVE Estimate  # The code is commented out to speed up compiling of the documentation.   # n=length(Y1) #  # start_time1 = Sys.time(); # B_hat_made1= made(X1, Y1, d=9, bw=1.42, lambda=0, B_mat=NULL,  #                   ytype=\"cat\", #                   method=list(opcg=\"cg\", made=\"cg\"), parallelize=T, r_mat=NULL, #                   control_list = list(print_iter=T, max_iter_made=10, max_iter=10)); #  #  # end_time1 = Sys.time(); end_time1 - start_time1; #  # # [1] \"2021-11-16 01:29:42 EST\" # # [1] \"MADE: euc_dist dist is\" \"0.35643430105849\"       \"1\"                      # # [1] \"MADE: euc_dist dist is\" \"0.272035485981084\"      \"2\"                      # # [1] \"MADE: euc_dist dist is\" \"0.216755954406661\"      \"3\"                      # # [1] \"MADE: euc_dist dist is\" \"0.178830986051283\"      \"4\"                      # # [1] \"MADE: euc_dist dist is\" \"0.14704936221923\"       \"5\"                      # # [1] \"MADE: euc_dist dist is\" \"0.127637259541059\"      \"6\"                      # # [1] \"MADE: euc_dist dist is\" \"0.109017830098574\"      \"7\"                      # # [1] \"MADE: euc_dist dist is\" \"0.0950488016216115\"     \"8\"                      # # [1] \"MADE: euc_dist dist is\" \"0.0786731005862281\"     \"9\"                      # # [1] \"MADE: euc_dist dist is\" \"0.0713249829082496\"     \"10\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0528917610230475\"     \"11\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0574909160293049\"     \"12\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0455500055604227\"     \"13\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0389085098426421\"     \"14\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0334073908028235\"     \"15\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0324984800139471\"     \"16\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0277371426019481\"     \"17\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0289097573928742\"     \"18\"                     # # [1] \"MADE: euc_dist dist is\" \"0.016257684884142\"      \"19\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0147111771624944\"     \"20\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0172574114573619\"     \"21\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0157875498645283\"     \"22\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0171129086535496\"     \"23\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0128793550013778\"     \"24\"                     # # [1] \"MADE: euc_dist dist is\" \"0.0126152738822231\"     \"25\"                     # # [1] \"0 - non-convergence\" # # Time difference of 3.811436 hours on 39 cores # #  # made_plot1_1=linearsdr:::ggplot_fsdr(Y1_test, t((X1_test_std)%*%b_hat_made1[,c(1,2)]), #                                      y_on_axis=F, #                                      ytype=\"multinomial\", #                                      h_lab='MADE 1', v_lab='MADE 2', #                                      main_lab= paste0('MADE'), size=2.5) #  # made_plot1_2=linearsdr:::ggplot_fsdr(Y1_test, t((X1_test_std)%*%b_hat_made1[,c(2,3)]), #                                      y_on_axis=F, #                                       ytype=\"multinomial\", #                                       h_lab='MADE 2', v_lab='MADE 3', #                                       main_lab= paste0('MADE'), size=2.5) #  # made_plot1_3=linearsdr:::ggplot_fsdr(Y1_test, t((X1_test_std)%*%b_hat_made1[,c(1,3)]), #                                      y_on_axis=F, #                                       ytype=\"multinomial\", #                                       h_lab='MADE 1', v_lab='MADE 3', #                                       main_lab= paste0('MADE'), size=2.5) #  # linearsdr:::save_sdr_plot(made_plot1_1, #                           filename = paste0('../man/figures/cat/ex1_made1.png'), #                           width = 800, height = 450, units = \"px\", pointsize = 12, #                           bg = \"white\",  res = 125)  knitr::include_graphics('../man/figures/cat/ex1_made1.png') knitr::include_graphics('../man/figures/cat/ex1_made2.png') knitr::include_graphics('../man/figures/cat/ex1_made3.png')"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"a-first-example","dir":"Articles","previous_headings":"","what":"A First Example","title":"Linear SDR for continuous Y","text":"first example, apply available methods continuous responses airfoil self-noise dataset. dataset available consists 5 predictors one response. variables measurements taken airfoil tests response sound pressure level. Two predictors discrete, violate theoretical assumptions methods applied, see methods can still work well. pairs plot illustrates discreteness predictors. randomly sample subset \\(500\\) estimation since computation MAVE can get quite slow.","code":"data('airfoil_datta', package=\"linearsdr\") # summary(airfoil_datta)  X1 = as.matrix(airfoil_datta[,1:5]) Y1 = c(airfoil_datta[,6])  pairs(X1)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"sliced-inverse-regression-sir","dir":"Articles","previous_headings":"A First Example","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for continuous Y","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Inverse Regression  b_hat_sir1 = sir(x=X1, y=Y1, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y1, t((X1)%*%b_hat_sir1[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='Y',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"sliced-average-variance-estimator-save","dir":"Articles","previous_headings":"A First Example","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for continuous Y","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Average Variance Estimator  b_hat_save1 = save_sdr(x=X1, y=Y1, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y1, t((X1)%*%b_hat_save1[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"directional-regression-dr","dir":"Articles","previous_headings":"A First Example","what":"Directional Regression (DR)","title":"Linear SDR for continuous Y","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Directional Regression  b_hat_dr1 = dr(x=X1, y=Y1, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y1, t((X1)%*%b_hat_dr1[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"outer-product-of-gradients-opg","dir":"Articles","previous_headings":"A First Example","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for continuous Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. bandwidth set \\(5\\) plot first sufficient predictor response. parallelize computation convenience .","code":"# OPG Estimate    X1_std=(sapply(1:dim(X1)[2], FUN= function(j)   center_cpp(X1[,j], NULL) ) )%*%matpower_cpp(cov((X1)) , -1/2);   b_hat_opg1 = opcg(x=X1_std, y=Y1, bw = 5, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )   linearsdr:::ggplot_fsdr(Y1, t((X1_std)%*%b_hat_opg1[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='Y',                         main_lab= paste0('OPG'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"minimum-average-variance-estimation-mave","dir":"Articles","previous_headings":"A First Example","what":"Minimum Average Variance Estimation (MAVE)","title":"Linear SDR for continuous Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. starting value \\(\\beta\\) matrix, use \\(p \\times d\\) matrix upper \\(d \\times d\\) block identity. bandwidth set \\(5\\) plot first sufficient predictor response.","code":"# MAVE Estimate  # The code is commented out to speed up compiling of the documentation.   # n=length(Y1) #  # start_time1 = Sys.time(); # b_hat_made1 = made(x=(X1_std), matrix(Y1, nrow = n, ncol=1), bw=5, d=2, #                   ytype='continuous', #                   method=list(opcg=\"cg\", made=\"cg\"), B_mat = NULL, #                   parallelize=T, #                   control_list=list(print_iter=T,  #                                     max_iter=5, max_iter_made=5) ) #  # end_time1 = Sys.time(); end_time1 - start_time1; # # # [1] \"MADE: euc_dist dist is\" \"0.331801141953547\"      \"1\" # # [1] \"MADE: euc_dist dist is\" \"0.0112107472862494\"     \"2\" # # [1] \"MADE: euc_dist dist is\" \"0.0102038542211796\"     \"3\" # # [1] \"MADE: euc_dist dist is\" \"0.00497290505768543\"    \"4\" # # [1] \"MADE: euc_dist dist is\" \"0.00261423206170839\"    \"5\" # # [1] \"0 - non-convergence\" # # Time difference of 22.35525 mins #   #  # mave_plot1=linearsdr:::ggplot_fsdr(Y1, t((X1_std)%*%b_hat_made1[,1]), y_on_axis=T, #                                   ytype=\"continuous\", #                                   h_lab='MAVE 1', v_lab='Y1', #                                   main_lab= paste0('MAVE'), size=2.5) #  # linearsdr:::save_sdr_plot(mave_plot1,filename = paste0('../man/figures/ex1_mave.png'), #                           width = 900, height = 450, units = \"px\", pointsize = 12, #                           bg = \"white\",  res = 100)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"regularized-inverse-methods","dir":"Articles","previous_headings":"","what":"Regularized Inverse Methods","title":"Linear SDR for continuous Y","text":"consider larger number predictors using Community Crime data available .","code":"data('crime_datta', package=\"linearsdr\") # dim(crime_datta)  X2 = crime_datta[, 1:( dim(crime_datta)[2] - 1) ] Y2 = crime_datta[, dim(crime_datta)[2]]  # p2 = dim(X2)[2]; n2=dim(X2)[1];"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"sliced-inverse-regression-sir-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for continuous Y","text":"SIR, use 5 slices plot first two sufficient predictors colored points representing value response \\(Y\\).","code":"# Sliced Inverse Regression  b_hat_sir2 = sir(x=X2, y=Y2, nslices = 5, d=2, ytype = \"continuous\",                 lambda = 0)$beta   linearsdr:::ggplot_fsdr(Y2, t((X2)%*%b_hat_sir2[,1:2]), y_on_axis=F,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='SIR 2',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"sliced-average-variance-estimator-save-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for continuous Y","text":"SAVE, use 5 slices consider first two sufficient predictors. consider regularized SAVE regularization parameter, \\(\\lambda\\) set 1. un-regularized regularized versions SAVE fail estimate directions dimension reduction.","code":"# Sliced Average Variance Estimator  b_hat_save2 = save_sdr(x=X2, y=Y2, nslices = 5, d=2, ytype = \"continuous\",               lambda = 0)$beta  b_hat_save2_reg = save_sdr(x=X2, y=Y2, nslices = 5, d=2, ytype = \"continuous\",                   lambda = 1)$beta  # par(mar=c(1, 0, 0, 1), mfrow=c(1,1))  linearsdr:::ggplot_fsdr(Y2, t((X2)%*%b_hat_save2[,1:2]), y_on_axis=F,                         ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='SAVE 2',                         main_lab= paste0('SAVE'), size=2.5)  linearsdr:::ggplot_fsdr(Y2, t((X2)%*%b_hat_save2_reg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='Y',                         main_lab= paste0('Regularized SAVE'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"directional-regression-dr-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Directional Regression (DR)","title":"Linear SDR for continuous Y","text":"Directional Regression, use 5 slices consider first two sufficient predictors. consider regularized SAVE regularization parameter, \\(\\lambda\\) set 1. un-regularized DR fails estimate directions dimension reduction, regularized DR able estimate one sufficient direction dimension reduction.","code":"# Directional Regression  b_hat_dr2 = dr(x=X2, y=Y2, nslices = 5, d=2, ytype = \"continuous\",               lambda = 0)$beta   b_hat_dr2_reg = dr(x=X2, y=Y2, nslices = 5, d=2, ytype = \"continuous\",                   lambda = 1)$beta   linearsdr:::ggplot_fsdr(Y2, t((X2)%*%b_hat_dr2[,1:2]), y_on_axis=F,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='DR 2',                         main_lab= paste0('DR'), size=1)  linearsdr:::ggplot_fsdr(Y2, t((X2)%*%b_hat_dr2_reg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('Regularized DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"outer-product-of-gradients-opg-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for continuous Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. bandwidth set \\(4\\) consider first two sufficient predictors response.","code":"X2_std=(sapply(1:dim(X2)[2], FUN= function(j)   center_cpp(X2[,j], NULL) ) )%*%matpower_cpp(cov((X2)) , -1/2);  b_hat_opg2 = opcg(x=X2_std, y=Y2, bw = 4, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )   linearsdr:::ggplot_fsdr(Y2, t((X2_std)%*%b_hat_opg2[,1:2]), y_on_axis=F,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='OPG 2',                         main_lab= paste0('OPG'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"minimum-average-variance-estimation-mave-1","dir":"Articles","previous_headings":"Regularized Inverse Methods","what":"Minimum Average Variance Estimation (MAVE)","title":"Linear SDR for continuous Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. starting value \\(\\beta\\) matrix, use OPG estimator since default \\(p \\times d\\) matrix upper block identity failed converge. bandwidth set \\(4\\) consider first two sufficient predictors response.","code":"# MAVE Estimate  # # The code is commented out to speed up compiling of the documentation.  #  # start_time1 = Sys.time(); # b_hat_made2 = made(x=(X2_std), Y2, bw=4, d=2, #                   ytype='continuous', #                   method=list(opcg=\"cg\", made=\"cg\"), B_mat = b_hat_opg2, #                   parallelize=T, #                   control_list=list(print_iter=T,  #                                     max_iter=5, max_iter_made=5) ) #  # end_time1 = Sys.time(); end_time1 - start_time1; ### For B_mat = NULL # [1] \"MADE: euc_dist dist is\" \"1.63713329933597e-15\"   # [3] \"1\"                      # >  # > end_time1 = Sys.time(); end_time1 - start_time1; # Time difference of 34.67113 mins #  ### For B_mat = b_hat_opg2 # [1] \"MADE: euc_dist dist is\" \"0.00134310136692448\"    # [3] \"1\"                      # [1] \"MADE: euc_dist dist is\" \"4.66580981942468e-15\"   # [3] \"2\"                      # >  # > end_time1 = Sys.time(); end_time1 - start_time1; # Time difference of 58.74611 mins #  # mave_plot2=linearsdr:::ggplot_fsdr(Y2, t((X2_std)%*%b_hat_made2[,1:2]), #                                    y_on_axis=F, #                                    ytype=\"continuous\", #                                    h_lab='MAVE 1', v_lab='Y', #                                    main_lab= paste0('MAVE'), size=2.5) #  # mave_plot2 #  # linearsdr:::save_sdr_plot(mave_plot2,filename = #                             paste0('../man/figures/ex2_mave.png'), #                                     width = 900, height = 450, units = \"px\", #                                     pointsize = 12, #                                     bg = \"white\",  res = 100)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"multivariate-y","dir":"Articles","previous_headings":"","what":"Multivariate \\(Y\\)","title":"Linear SDR for continuous Y","text":"demonstrating linear SDR methods multivariate responses, consider Energy Efficiency dataset available . 8 predictors, pairs plot, can see number discrete. response bivariate, first response heating load, second response cooling load. inverse SDR methods, apply SDR methods component-wise response. first sufficient predictor component response taken sufficient dimension reduction direction. similar approach Cook Setodji. forward methods, consider entire response simultaneously.","code":"data('energy_datta', package=\"linearsdr\") # summary(energy_datta) # dim(energy_datta)  X3 = as.matrix(energy_datta[,1:8]) Y3 = cbind(energy_datta$Y1,energy_datta$Y2)  Y3_1 = as.numeric(energy_datta$Y1) Y3_2 = as.numeric(energy_datta$Y2)  pairs(X3)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"sliced-inverse-regression-sir-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for continuous Y","text":"SIR, use 5 slices estimate 2 sufficient directions per component. take first direction component get two sufficient predictors.","code":"b_hat_sir3_1 = sir(x=X3, y=Y3_1, nslices = 5, d=2,                     ytype = \"continuous\" )$beta  b_hat_sir3_2 = sir(x=X3, y=Y3_2, nslices = 5, d=2,                     ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y3_1, t((X3)%*%b_hat_sir3_1[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='Y1',                         main_lab= paste0('SIR'), size=3)  linearsdr:::ggplot_fsdr(Y3_2, t((X3)%*%b_hat_sir3_2[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='Y2',                         main_lab= paste0('SIR'), size=3)  linearsdr:::ggplot_fsdr(Y3_1,                          t((X3)%*%cbind(b_hat_sir3_1[,1], b_hat_sir3_2[,1])),                         y_on_axis=F,                          ytype=\"continuous\",                         h_lab='SIR 1', v_lab='SIR 2',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"sliced-average-variance-estimator-save-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for continuous Y","text":"SAVE, use 5 slices, SIR, plot two estimated sufficient predictors.","code":"# Sliced Average Variance Estimator  b_hat_save3_1 = save_sdr(x=X3, y=Y3_1, nslices = 5, d=2, ytype = \"continuous\" )$beta  b_hat_save3_2 = save_sdr(x=X3, y=Y3_2, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y3_1, t((X3)%*%b_hat_save3_1[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='Y1',                         main_lab= paste0('SAVE'), size=2.5)  linearsdr:::ggplot_fsdr(Y3_2, t((X3)%*%b_hat_save3_2[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='Y2',                         main_lab= paste0('SAVE'), size=2.5)   linearsdr:::ggplot_fsdr(Y3_1,                          t((X3)%*%cbind(b_hat_save3_1[,1], b_hat_save3_2[,1])),                         y_on_axis=T,                          ytype=\"continuous\",                         h_lab='SAVE 1', v_lab='SAVE 2',                         main_lab= paste0('SAVE'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"directional-regression-dr-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Directional Regression (DR)","title":"Linear SDR for continuous Y","text":"","code":"b_hat_dr3_1 = dr(x=X3, y=Y3_1, nslices = 5, d=2, ytype = \"continuous\" )$beta  b_hat_dr3_2 = dr(x=X3, y=Y3_2, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y3_1, t((X3)%*%b_hat_dr3_1[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y1',                         main_lab= paste0('DR'), size=3)  linearsdr:::ggplot_fsdr(Y3_2, t((X3)%*%b_hat_dr3_2[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y2',                         main_lab= paste0('DR'), size=3)   linearsdr:::ggplot_fsdr(Y3_1,                          t((X3)%*%cbind(b_hat_dr3_1[,1], b_hat_dr3_2[,1]) ),                         y_on_axis=F,                         ytype=\"continuous\",                         h_lab='DR 1', v_lab='DR 2',                         main_lab= paste0('DR'), size=3)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"outer-product-of-gradients-opg-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for continuous Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. bandwidth set \\(1.5\\) consider first two sufficient predictors bivariate response.","code":"# Parallelization not run because of computational time.   X3_std=(sapply(1:dim(X3)[2], FUN= function(j)   center_cpp(X3[,j], NULL) ) )%*%matpower_cpp(cov((X3)) , -1/2);  b_hat_opg3 = opcg(x=X3_std, y=Y3, bw = 1.5, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )  linearsdr:::ggplot_fsdr(Y3_1, t((X3_std)%*%b_hat_opg3[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='Y1',                         main_lab= paste0('OPG'), size=3)   linearsdr:::ggplot_fsdr(Y3_2, t((X3_std)%*%b_hat_opg3[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='Y2',                         main_lab= paste0('OPG'), size=3)   linearsdr:::ggplot_fsdr(Y3_2, t((X3_std)%*%b_hat_opg3[,1:2]), y_on_axis=F,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='OPG 2',                         main_lab= paste0('OPG'), size=3)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_cts.html","id":"minimum-average-variance-estimation-mave-2","dir":"Articles","previous_headings":"Multivariate \\(Y\\)","what":"Minimum Average Variance Estimation (MAVE)","title":"Linear SDR for continuous Y","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. starting value \\(\\beta\\) matrix, use default \\(p \\times d\\) matrix upper block identity. bandwidth set \\(1.5\\) consider first two sufficient predictors bivariate response.","code":"# MAVE Estimate  # # The code is commented out to speed up compiling of the documentation.  #  # start_time1 = Sys.time(); # b_hat_made3 = made(x=(X3_std), Y3, bw=1.5, d=2, #                   ytype='continuous', #                   method=list(opcg=\"cg\", made=\"cg\"), B_mat = NULL, #                   parallelize=T, #                   control_list=list(print_iter=T,  #                                     max_iter=5, max_iter_made=5) ) # end_time1 = Sys.time(); end_time1 - start_time1; #### For B_mat = NULL # [1] \"MADE: euc_dist dist is\" \"0.131175506718606\"      # [3] \"1\"                      # [1] \"MADE: euc_dist dist is\" \"0.0436618536536756\"     # [3] \"2\"                      # [1] \"MADE: euc_dist dist is\" \"0.0152859202750813\"     # [3] \"3\"                      # [1] \"MADE: euc_dist dist is\" \"0.00468325544335494\"    # [3] \"4\"                      # [1] \"MADE: euc_dist dist is\" \"0.000958786277942029\"   # [3] \"5\"                      # [1] \"0 - non-convergence\" # > end_time1 = Sys.time(); end_time1 - start_time1; # Time difference of 18.72442 mins # #  # mave_plot3_1=linearsdr:::ggplot_fsdr(Y3_1, t((X3_std)%*%b_hat_made3[,1]), #                                      y_on_axis=T, #                                      ytype=\"continuous\", #                                      h_lab='MAVE 1', v_lab='Y1', #                                      main_lab= paste0('MAVE'), size=2.5) #  # mave_plot3_2=linearsdr:::ggplot_fsdr(Y3_2, t((X3_std)%*%b_hat_made3[,1]), #                                      y_on_axis=T, #                                      ytype=\"continuous\", #                                      h_lab='MAVE 1', v_lab='Y2', #                                      main_lab= paste0('MAVE'), size=2.5) #  # mave_plot3=linearsdr:::ggplot_fsdr(Y3_1, t((X3_std)%*%b_hat_made3[,1:2]), #                                    y_on_axis=F, #                                    ytype=\"continuous\", #                                    h_lab='MAVE 1', v_lab='MAVE 2', #                                    main_lab= paste0('MAVE'), size=2.5) #  # linearsdr:::ggplot_fsdr(Y3_2, t((X3_std)%*%b_hat_made3[,1]), #                                    y_on_axis=T, #                                    ytype=\"continuous\", #                                    h_lab='MAVE 1', v_lab='MAVE 2', #                                    main_lab= paste0('MAVE'), size=2.5) #  # linearsdr:::save_sdr_plot(mave_plot3_2,filename = #                             paste0('../man/figures/ex3_mave2.png'), #                                     width = 900, height = 450, units = \"px\", #                                     pointsize = 12, #                                     bg = \"white\",  res = 100)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/Tuning OPCG.html","id":"r-markdown","dir":"Articles","previous_headings":"","what":"R Markdown","title":"Tuning OPCG","text":"R Markdown document. Markdown simple formatting syntax authoring HTML, PDF, MS Word documents. details using R Markdown see http://rmarkdown.rstudio.com. click Knit button document generated includes content well output embedded R code chunks within document. can embed R code chunk like :","code":"summary(cars) ##      speed           dist        ##  Min.   : 4.0   Min.   :  2.00   ##  1st Qu.:12.0   1st Qu.: 26.00   ##  Median :15.0   Median : 36.00   ##  Mean   :15.4   Mean   : 42.98   ##  3rd Qu.:19.0   3rd Qu.: 56.00   ##  Max.   :25.0   Max.   :120.00"},{"path":"https://github.com/HarrisQ/linearsdr/articles/Tuning OPCG.html","id":"including-plots","dir":"Articles","previous_headings":"","what":"Including Plots","title":"Tuning OPCG","text":"can also embed plots, example: Note echo = FALSE parameter added code chunk prevent printing R code generated plot.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors","text":"Harris Quach. Author, maintainer.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/index.html","id":"linear-sufficient-dimension-reduction","dir":"","previous_headings":"","what":"Linear SDR","title":"Linear SDR","text":"‘linearsdr’ package contains popular methods sufficient dimension reduction well recent methods forthcoming paper “Generalized Forward Sufficient Dimension Reduction Categorical Ordinal Responses”.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Linear SDR","text":"package can installed running:","code":"# Install development version from GitHub devtools::install_github(\"HarrisQ/linearsdr\")"},{"path":"https://github.com/HarrisQ/linearsdr/index.html","id":"current-state-of-package","dir":"","previous_headings":"","what":"Current State of Package:","title":"Linear SDR","text":"Forward Linear SDR methods: OPG/OPCG, MADE, Tuning OPCG Inverse Linear SDR methods: SIR, DR, SAVE options regularization Write Vignettes/Examples Finish code MAVE code Finish code Multivariate Continuous Response Write Documentation functions Clean OPG/OPCG MAVE/MADE code Limit function exports just ones people use","code":""},{"path":"https://github.com/HarrisQ/linearsdr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Harris Quach Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — airfoil_datta","title":"Scores of Group A and Group B — airfoil_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — airfoil_datta","text":"","code":"airfoil_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — airfoil_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — airfoil_datta","text":"https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — crime_datta","title":"Scores of Group A and Group B — crime_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — crime_datta","text":"","code":"crime_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — crime_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — crime_datta","text":"https://archive.ics.uci.edu/ml/datasets/Communities++Crime","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Directional Regression — dr","title":"Directional Regression — dr","text":"conducts directional regression (DR) Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Directional Regression — dr","text":"","code":"dr(x, y, nslices, d, ytype, lambda = 0)"},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Directional Regression — dr","text":"x 'n x p' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Directional Regression — dr","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix DR; used functions order determination.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Directional Regression — dr","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005). apply idea SAVE, find context analogous regularization works.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — energy_datta","title":"Scores of Group A and Group B — energy_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — energy_datta","text":"","code":"energy_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — energy_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — energy_datta","text":"https://archive.ics.uci.edu/ml/datasets/Energy+efficiency","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — fish_datta","title":"Scores of Group A and Group B — fish_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — fish_datta","text":"","code":"fish_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — fish_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — fish_datta","text":"https://archive.ics.uci.edu/ml/datasets/QSAR+fish+toxicity","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"K-fold Tuning with K-means — kfold_km_tuning","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"implements tuning procedure SDR classification problems forth coming paper Quach Li (2021).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"","code":"kfold_km_tuning(   h_list,   k,   x_datta,   y_datta,   d,   ytype,   class_labels,   n_cpc,   method = \"newton\",   std = \"none\",   parallelize = F,   control_list = list(),   iter.max = 100,   nstart = 100 )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"h_list  k  x_datta  y_datta  d specified reduced dimension ytype specify response 'continuous', 'multinomial', 'ordinal' class_labels  n_cpc  method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. control_list list control parameters Newton-Raphson Conjugate Gradient methods iter.max  nstart","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"list containing estimate candidate matrix. opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":null,"dir":"Reference","previous_headings":"","what":"Minimum Average Deviance Estimation — made","title":"Minimum Average Deviance Estimation — made","text":"implements Outer Product Canonical Gradients (OPCG) forth coming paper Quach Li (2021).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Minimum Average Deviance Estimation — made","text":"","code":"made(   x,   y,   d,   bw,   lambda = 0,   B_mat = NULL,   ytype = \"continuous\",   method = list(opcg = \"newton\", made = \"newton\"),   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minimum Average Deviance Estimation — made","text":"d specified reduced dimension bw bandwidth parameter kernel; default kernel gaussian B_mat  ytype specify response 'continuous', 'cat', 'ord-cat' method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. r_mat  control_list list control parameters Newton-Raphson Conjugate Gradient methods opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM. x_matrix 'pxn' matrix predictors; y_matrix 'mxn' matrix response","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minimum Average Deviance Estimation — made","text":"list containing estimate candidate matrix. opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Minimum Average Deviance Estimation — made","text":"version MADE differs Adragni currently available continuous, cat, ord-cat response far. scalar continuous response, estimation identical OPG. kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Outer Product of Canonical Gradients — opcg","title":"Outer Product of Canonical Gradients — opcg","text":"implements Outer Product Canonical Gradients (OPCG) forth coming paper Quach Li (2021).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Outer Product of Canonical Gradients — opcg","text":"","code":"opcg(   x,   y,   d,   bw,   lambda = 0,   ytype = \"continuous\",   method = \"newton\",   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outer Product of Canonical Gradients — opcg","text":"x 'nxp' matrix predictors; y 'nxm' response d specified reduced dimension bw bandwidth parameter kernel; default kernel gaussian ytype specify response 'continuous', 'multinomial', 'ordinal' method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. control_list list control parameters Newton-Raphson Conjugate Gradient methods opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outer Product of Canonical Gradients — opcg","text":"'pxd' matrix estimates basis central subspace based estimated local gradients","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Outer Product of Canonical Gradients — opcg","text":"kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":null,"dir":"Reference","previous_headings":"","what":"OPCG-MADE - Local gradient estimation — opcg_made","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"internal function called OPCG. MADE also uses function OPCG-step. estimates local intercept slope coefficients.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"","code":"opcg_made(   x_matrix,   y_matrix,   bw,   lambda,   B_mat = NULL,   ytype = \"continuous\",   method = \"newton\",   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"x_matrix 'nxp' matrix predictors; y_matrix 'nxm' response; bw bandwidth parameter kernel; default kernel gaussian lambda L2 penalty term negative log-likelihood B_mat fixed coefficient matrix MADE-step MADE; needed OPCG, .e. set identity ytype response type; continuous, categorical ordinal method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. r_mat 'pxd' matrix refining weights rOPCG rMADE control_list list control parameters Newton-Raphson Conjugate Gradient methods","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"ahat - List estimated local intercepts Dhat - List estimated local slopes/gradients Dhat_ls - List initial values local slopes/gradients; least squares, Dhat weights - kernel weights used local-linear estimation;","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/pendigits_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — pendigits_datta","title":"Scores of Group A and Group B — pendigits_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/pendigits_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — pendigits_datta","text":"","code":"pendigits_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/pendigits_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — pendigits_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/pendigits_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — pendigits_datta","text":"https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition++Handwritten+Digits","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Sliced Average Variance Estimation — save_sdr","title":"Sliced Average Variance Estimation — save_sdr","text":"conducts sliced average variance estimation (SAVE) Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Sliced Average Variance Estimation — save_sdr","text":"","code":"save_sdr(x, y, nslices, d, ytype, std = T, lambda = 0)"},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sliced Average Variance Estimation — save_sdr","text":"x 'n x p' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sliced Average Variance Estimation — save_sdr","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix SAVE; used functions order determination.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sliced Average Variance Estimation — save_sdr","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005). apply idea SAVE, find context analogous regularization works.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":null,"dir":"Reference","previous_headings":"","what":"Sliced Inverse Regression — sir","title":"Sliced Inverse Regression — sir","text":"conducts sliced inverse regression Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Sliced Inverse Regression — sir","text":"","code":"sir(x, y, nslices, d, ytype, lambda = 0)"},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sliced Inverse Regression — sir","text":"x 'n x p' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sliced Inverse Regression — sir","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix SIR; used functions order determination.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sliced Inverse Regression — sir","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — supercond_datta","title":"Scores of Group A and Group B — supercond_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — supercond_datta","text":"","code":"supercond_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — supercond_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — supercond_datta","text":"https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data","code":""},{"path":[]}]
