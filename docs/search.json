[{"path":[]},{"path":[]},{"path":[]},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"airfoil-noise","dir":"Articles","previous_headings":"","what":"Airfoil Noise","title":"Linear SDR for scalar Y, Ex1","text":"airfoil dataset available consists 5 predictors one response. variables measurements taken airfoil tests response sound pressure level. Two predictors discrete, violate theoretical assumptions methods applied, see methods can still work well. pairs plot illustrates discreteness predictors. randomly sample subset \\(500\\) estimation since computation MAVE can get quite slow.","code":"library(ggplot2) #> Warning: package 'ggplot2' was built under R version 4.0.5 library(linearsdr) data('airfoil_datta', package=\"linearsdr\") # summary(airfoil_datta)  X = as.matrix(airfoil_datta[,1:5]) Y = c(airfoil_datta[,6])  pairs(X)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-inverse-regression-sir","dir":"Articles","previous_headings":"Airfoil Noise","what":"Sliced Inverse Regression (SIR)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Inverse Regression  b_hat_sir = sir(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_sir[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='SIR 1', v_lab='Y',                         main_lab= paste0('SIR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"sliced-average-variance-estimator-save","dir":"Articles","previous_headings":"Airfoil Noise","what":"Sliced Average Variance Estimator (SAVE)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Sliced Average Variance Estimator  b_hat_save = save_sdr(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_save[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"directional-regression-dr","dir":"Articles","previous_headings":"Airfoil Noise","what":"Directional Regression (DR)","title":"Linear SDR for scalar Y, Ex1","text":"choose number slices \\(5\\) plot first sufficient predictor response.","code":"# Directional Regression  b_hat_dr = dr(x=X, y=Y, nslices = 5, d=2, ytype = \"continuous\" )$beta   linearsdr:::ggplot_fsdr(Y, t((X)%*%b_hat_dr[,1]), y_on_axis=T,                          ytype=\"continuous\",                         h_lab='DR 1', v_lab='Y',                         main_lab= paste0('DR'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"outer-product-of-gradients-opg","dir":"Articles","previous_headings":"Airfoil Noise","what":"Outer Product of Gradients (OPG)","title":"Linear SDR for scalar Y, Ex1","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. bandwidth set \\(5\\) plot first sufficient predictor response. parallelize computation convenience .","code":"# OPG Estimate   library(\"doParallel\") #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel library(\"foreach\") print( paste( as.character(detectCores()), \"cores detected\" ) ); #> [1] \"16 cores detected\" # Create cluster with desired number of cores cl <- makePSOCKcluster(detectCores()-1) # Register cluster doParallel::registerDoParallel(cl) # Find out how many cores are being used print( paste( as.character(getDoParWorkers() ), \"cores registered\" ) ) #> [1] \"15 cores registered\" # stopCluster(cl)  X_std=(sapply(1:dim(X)[2], FUN= function(j)   center_cpp(X[,j], NULL) ) )%*%matpower_cpp(cov((X)) , -1/2);   b_hat_opg = opcg(x=X_std, y=Y, bw = 5, d=2, ytype = \"continuous\",                   method= \"cg\", parallelize = T )   linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_opg[,1]), y_on_axis=T,                         ytype=\"continuous\",                         h_lab='OPG 1', v_lab='Y',                         main_lab= paste0('OPG'), size=2.5)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/sdr_airfoil.html","id":"minimum-average-variance-estimation-mave","dir":"Articles","previous_headings":"Airfoil Noise","what":"Minimum Average Variance Estimation (MAVE)","title":"Linear SDR for scalar Y, Ex1","text":"use Gaussian kernel local linear weights standardize predictors kernel choice appropriate. starting value \\(\\beta\\) matrix, use \\(p \\times d\\) matrix upper \\(d \\times d\\) block identity. bandwidth set \\(5\\) plot first sufficient predictor response.","code":"# MAVE Estimate  # The code is commented out to speed up compiling of the documentation.   # n=length(Y) #  # start_time1 = Sys.time(); # b_hat_made = made(x=(X_std), matrix(Y, nrow = n, ncol=1), bw=5, d=2, #                   ytype='continuous', #                   method=list(opcg=\"cg\", made=\"cg\"), B_mat = NULL, #                   parallelize=T, #                   control_list=list(print_iter=T, max_iter_made=5) ) #  # end_time1 = Sys.time(); end_time1 - start_time1; # # # [1] \"MADE: euc_dist dist is\" \"0.331801141953547\"      \"1\" # # [1] \"MADE: euc_dist dist is\" \"0.0112107472862494\"     \"2\" # # [1] \"MADE: euc_dist dist is\" \"0.0102038542211796\"     \"3\" # # [1] \"MADE: euc_dist dist is\" \"0.00497290505768543\"    \"4\" # # [1] \"MADE: euc_dist dist is\" \"0.00261423206170839\"    \"5\" # # [1] \"0 - non-convergence\" # # Time difference of 22.35525 mins #   #  # plot saved image instead # mave_plot=linearsdr:::ggplot_fsdr(Y, t((X_std)%*%b_hat_made[,1]), y_on_axis=T, #                                   ytype=\"continuous\", #                                   h_lab='MAVE 1', v_lab='Y', #                                   main_lab= paste0('MAVE'), size=2.5) #  # linearsdr:::save_sdr_plot(mave_plot,filename = paste0('../man/figures/ex1_mave.png'), #                           width = 900, height = 450, units = \"px\", pointsize = 12, #                           bg = \"white\",  res = 100)"},{"path":"https://github.com/HarrisQ/linearsdr/articles/Tuning OPCG.html","id":"r-markdown","dir":"Articles","previous_headings":"","what":"R Markdown","title":"Tuning OPCG","text":"R Markdown document. Markdown simple formatting syntax authoring HTML, PDF, MS Word documents. details using R Markdown see http://rmarkdown.rstudio.com. click Knit button document generated includes content well output embedded R code chunks within document. can embed R code chunk like :","code":"summary(cars) ##      speed           dist        ##  Min.   : 4.0   Min.   :  2.00   ##  1st Qu.:12.0   1st Qu.: 26.00   ##  Median :15.0   Median : 36.00   ##  Mean   :15.4   Mean   : 42.98   ##  3rd Qu.:19.0   3rd Qu.: 56.00   ##  Max.   :25.0   Max.   :120.00"},{"path":"https://github.com/HarrisQ/linearsdr/articles/Tuning OPCG.html","id":"including-plots","dir":"Articles","previous_headings":"","what":"Including Plots","title":"Tuning OPCG","text":"can also embed plots, example: Note echo = FALSE parameter added code chunk prevent printing R code generated plot.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors","text":"Harris Quach. Author, maintainer.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/index.html","id":"linear-sufficient-dimension-reduction","dir":"","previous_headings":"","what":"Linear SDR","title":"Linear SDR","text":"‘linearsdr’ package contains popular methods sufficient dimension reduction well recent methods forthcoming paper “Generalized Forward Sufficient Dimension Reduction Categorical Ordinal Responses”.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Linear SDR","text":"package can installed running:","code":"# Install development version from GitHub devtools::install_github(\"HarrisQ/linearsdr\")"},{"path":"https://github.com/HarrisQ/linearsdr/index.html","id":"current-state-of-package","dir":"","previous_headings":"","what":"Current State of Package:","title":"Linear SDR","text":"Forward Linear SDR methods: OPG/OPCG, MADE, Tuning OPCG Inverse Linear SDR methods: SIR, DR, SAVE options regularization Write Vignettes/Examples Finish code MAVE code Finish code Multivariate Continuous Response Write Documentation functions Clean OPG/OPCG MAVE/MADE code Limit function exports just ones people use","code":""},{"path":"https://github.com/HarrisQ/linearsdr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Harris Quach Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — airfoil_datta","title":"Scores of Group A and Group B — airfoil_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — airfoil_datta","text":"","code":"airfoil_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — airfoil_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/airfoil_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — airfoil_datta","text":"https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — crime_datta","title":"Scores of Group A and Group B — crime_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — crime_datta","text":"","code":"crime_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — crime_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/crime_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — crime_datta","text":"https://archive.ics.uci.edu/ml/datasets/Communities++Crime","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Directional Regression — dr","title":"Directional Regression — dr","text":"conducts directional regression (DR) Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Directional Regression — dr","text":"","code":"dr(x, y, nslices, d, ytype, lambda = 0)"},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Directional Regression — dr","text":"x 'n x p' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Directional Regression — dr","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix DR; used functions order determination.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/dr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Directional Regression — dr","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005). apply idea SAVE, find context analogous regularization works.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — energy_datta","title":"Scores of Group A and Group B — energy_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — energy_datta","text":"","code":"energy_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — energy_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/energy_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — energy_datta","text":"https://archive.ics.uci.edu/ml/datasets/Energy+efficiency","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — fish_datta","title":"Scores of Group A and Group B — fish_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — fish_datta","text":"","code":"fish_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — fish_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/fish_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — fish_datta","text":"https://archive.ics.uci.edu/ml/datasets/QSAR+fish+toxicity","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"K-fold Tuning with K-means — kfold_km_tuning","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"implements tuning procedure SDR classification problems forth coming paper Quach Li (2021).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"","code":"kfold_km_tuning(   h_list,   k,   x_datta,   y_datta,   d,   ytype,   class_labels,   n_cpc,   method = \"newton\",   std = \"none\",   parallelize = F,   control_list = list(),   iter.max = 100,   nstart = 100 )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"h_list  k  x_datta  y_datta  d specified reduced dimension ytype specify response 'continuous', 'multinomial', 'ordinal' class_labels  n_cpc  method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. control_list list control parameters Newton-Raphson Conjugate Gradient methods iter.max  nstart","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"list containing estimate candidate matrix. opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/kfold_km_tuning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"K-fold Tuning with K-means — kfold_km_tuning","text":"kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":null,"dir":"Reference","previous_headings":"","what":"Minimum Average Deviance Estimation — made","title":"Minimum Average Deviance Estimation — made","text":"implements Outer Product Canonical Gradients (OPCG) forth coming paper Quach Li (2021).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Minimum Average Deviance Estimation — made","text":"","code":"made(   x,   y,   d,   bw,   lambda = 0,   B_mat = NULL,   ytype = \"continuous\",   method = list(opcg = \"newton\", made = \"newton\"),   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Minimum Average Deviance Estimation — made","text":"d specified reduced dimension bw bandwidth parameter kernel; default kernel gaussian B_mat  ytype specify response 'continuous', 'cat', 'ord-cat' method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. r_mat  control_list list control parameters Newton-Raphson Conjugate Gradient methods opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM. x_matrix 'pxn' matrix predictors; y_matrix 'mxn' matrix response","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Minimum Average Deviance Estimation — made","text":"list containing estimate candidate matrix. opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/made.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Minimum Average Deviance Estimation — made","text":"version MADE differs Adragni currently available continuous, cat, ord-cat response far. scalar continuous response, estimation identical OPG. kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Outer Product of Canonical Gradients — opcg","title":"Outer Product of Canonical Gradients — opcg","text":"implements Outer Product Canonical Gradients (OPCG) forth coming paper Quach Li (2021).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Outer Product of Canonical Gradients — opcg","text":"","code":"opcg(   x,   y,   d,   bw,   lambda = 0,   ytype = \"continuous\",   method = \"newton\",   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outer Product of Canonical Gradients — opcg","text":"x 'nxp' matrix predictors; y 'nxm' response d specified reduced dimension bw bandwidth parameter kernel; default kernel gaussian ytype specify response 'continuous', 'multinomial', 'ordinal' method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. control_list list control parameters Newton-Raphson Conjugate Gradient methods opcg - 'pxd' matrix estimates basis central subspace. opcg_wls - 'pxd' matrix estimates basis central subspace based initial value optimization problem; useful examining bad starting values. cand_mat - list contains candidate matrix OPCG initial value; used functions order determination gradients - estimated local gradients; used regularization OPCG weights - kernel weights local-linear GLM.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outer Product of Canonical Gradients — opcg","text":"'pxd' matrix estimates basis central subspace based estimated local gradients","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Outer Product of Canonical Gradients — opcg","text":"kernel local linear regression fixed gaussian kernel. large 'p', strongly recommend using Conjugate Gradients implement, setting method=\"cg\". method=\"cg\", hybrid conjugate gradient Dai Yuan implemented, armijo rule implemented backtracking, like Bertsekas' \"Convex Optimization Algorithms\". weak Wolfe condition can also enforced adding setting c_wolfe > 0 control_list, since c_wolfe usually set 0.1 (Wikipedia) drastically slows algorithm relative newton small moderate p, leave default enforcing Wolfe condition, since assume link function gives us close enough initial point local convergence satisfactory. initial values suspect, maybe enforcing Wolfe condition reasonable trade-.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":null,"dir":"Reference","previous_headings":"","what":"OPCG-MADE - Local gradient estimation — opcg_made","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"internal function called OPCG. MADE also uses function OPCG-step. estimates local intercept slope coefficients.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"","code":"opcg_made(   x_matrix,   y_matrix,   bw,   lambda,   B_mat = NULL,   ytype = \"continuous\",   method = \"newton\",   parallelize = F,   r_mat = NULL,   control_list = list() )"},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"x_matrix 'nxp' matrix predictors; y_matrix 'nxm' response; bw bandwidth parameter kernel; default kernel gaussian lambda L2 penalty term negative log-likelihood B_mat fixed coefficient matrix MADE-step MADE; needed OPCG, .e. set identity ytype response type; continuous, categorical ordinal method \"newton\" \"cg\" methods; carrying optimization using standard newton-raphson (.e. Fisher Scoring) using Congugate Gradients parallelize Default False; run parallel, need foreach parallel backend loaded; parallelization strongly recommended encouraged. r_mat 'pxd' matrix refining weights rOPCG rMADE control_list list control parameters Newton-Raphson Conjugate Gradient methods","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/opcg_made.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OPCG-MADE - Local gradient estimation — opcg_made","text":"ahat - List estimated local intercepts Dhat - List estimated local slopes/gradients Dhat_ls - List initial values local slopes/gradients; least squares, Dhat weights - kernel weights used local-linear estimation;","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Sliced Average Variance Estimation — save_sdr","title":"Sliced Average Variance Estimation — save_sdr","text":"conducts sliced average variance estimation (SAVE) Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Sliced Average Variance Estimation — save_sdr","text":"","code":"save_sdr(x, y, nslices, d, ytype, std = T, lambda = 0)"},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sliced Average Variance Estimation — save_sdr","text":"x 'n x p' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sliced Average Variance Estimation — save_sdr","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix SAVE; used functions order determination.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/save_sdr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sliced Average Variance Estimation — save_sdr","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005). apply idea SAVE, find context analogous regularization works.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":null,"dir":"Reference","previous_headings":"","what":"Sliced Inverse Regression — sir","title":"Sliced Inverse Regression — sir","text":"conducts sliced inverse regression Li (2018) modifications improve speed allow option standardizing regularizing","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Sliced Inverse Regression — sir","text":"","code":"sir(x, y, nslices, d, ytype, lambda = 0)"},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sliced Inverse Regression — sir","text":"x 'n x p' matrix predictors; n sample size, p dimension y scalar response nslices specify number slices conduct; d specify reduced dimension ytype specify response 'continuous' 'categorical' lambda L2 Tikonov regularizer sample covariance matrix; default '0', .e. regularization","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sliced Inverse Regression — sir","text":"list containing estimate candidate matrix. beta - 'pxd' matrix estimates basis central subspace. cand_mat - candidate matrix SIR; used functions order determination.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/sir.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sliced Inverse Regression — sir","text":"Standardizing default necessary recovering properly scaled central subspace. However, certain contexts, standardization necessary, leave option open practitioner. L2-regularization option corresponds SIR regularization idea Zhang et al.(2005).","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Scores of Group A and Group B — supercond_datta","title":"Scores of Group A and Group B — supercond_datta","text":"data set scores two groups.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":null,"dir":"Reference","previous_headings":"","what":"Usage","title":"Scores of Group A and Group B — supercond_datta","text":"","code":"supercond_datta"},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scores of Group A and Group B — supercond_datta","text":"data frame 60 rows 2 variables: group Participant's group, B. score participant's score hypothetical task.","code":""},{"path":"https://github.com/HarrisQ/linearsdr/reference/supercond_datta.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Scores of Group A and Group B — supercond_datta","text":"https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data","code":""}]
